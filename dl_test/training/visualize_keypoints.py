import torch
import numpy as np
import os
from PIL import Image
import cv2 # OpenCVëŠ” ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ì‹œê°í™”ì— ìœ ìš©í•©ë‹ˆë‹¤.
import mmcv

# --- MMPose 1.3.2+ ë²„ì „ì— ë§ëŠ” ì„í¬íŠ¸ ---
from mmpose.apis import init_model 
from mmpose.visualization import PoseLocalVisualizer 
# from mmpose.datasets import DatasetInfo # <-- ì´ ì¤„ì€ ì‚­ì œí•©ë‹ˆë‹¤!
from mmengine import Config

# --- MMPose 1.3.2+ ë²„ì „ìš© êµ¬ì¡°ì²´ ì„í¬íŠ¸ ---
from mmpose.structures import PoseDataSample
from mmengine.structures import InstanceData # bboxë¥¼ ë‹´ê¸° ìœ„í•´ í•„ìš”

# --- SimCLR íŠ¹ì§• ê²€ìƒ‰ í•¨ìˆ˜ ì„í¬íŠ¸ (ì´ì „ ë‹¨ê³„ì—ì„œ ì‘ì„±í•œ íŒŒì¼) ---
from search_similar_dogs import search_similar_dogs 

# --- AP-10K ëª¨ë¸ ë° ì„¤ì • ê²½ë¡œ ---
MMPose_ROOT = 'C:/dl_final/mm_pose/mmpose' # ì—¬ê¸°ê°€ ì‚¬ìš©ì ì´ë¯¸ì§€ ì—…ë¡œë“œ ê²½ë¡œ

# ì„¤ì • íŒŒì¼ ê²½ë¡œ (ì‚¬ìš©ìë‹˜ê»˜ì„œ ì œê³µí•´ì£¼ì‹  ì •í™•í•œ ê²½ë¡œì™€ íŒŒì¼ëª… ì‚¬ìš©)
AP10K_CONFIG_FILE = os.path.join(MMPose_ROOT, 'configs', 'animal_2d_keypoint', 
                                 'topdown_heatmap', 'ap10k', 'td-hm_hrnet-w32_8xb64-210e_ap10k-256x256.py') # ğŸš¨ ì´ ë¶€ë¶„ì„ ë°˜ë“œì‹œ ìˆ˜ì •í•˜ì„¸ìš”! ğŸš¨

# ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ (ì‚¬ìš©ìë‹˜ê»˜ì„œ ì œê³µí•´ì£¼ì‹  ì •í™•í•œ íŒŒì¼ëª… ì‚¬ìš©)
AP10K_CHECKPOINT_FILE = os.path.join(MMPose_ROOT, 'checkpoints', 'hrnet_w32_ap10k_256x256-18aac840_20211029.pth') # ğŸš¨ ì´ ë¶€ë¶„ì„ ë°˜ë“œì‹œ ìˆ˜ì •í•˜ì„¸ìš”! ğŸš¨

# --- SimCLR ê´€ë ¨ ì„¤ì • (ì´ì „ íŒŒì¼ë“¤ì—ì„œ ê°€ì ¸ì˜´) ---
SIMCLR_MODEL_PATH = 'models/simclr_vit_dog_model.pth' 
SIMCLR_OUT_DIM = 128
SIMCLR_IMAGE_SIZE = 224
DB_FEATURES_FILE = 'db_features.npy' 
DB_IMAGE_PATHS_FILE = 'db_image_paths.npy'

# ì¿¼ë¦¬ ì´ë¯¸ì§€ ê²½ë¡œ (í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì„¤ì •, ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œëŠ” ì‚¬ìš©ì ì…ë ¥ ë°›ìŒ)
# ğŸš¨ğŸš¨ğŸš¨ ì´ ê²½ë¡œë„ ë³¸ì¸ì˜ ì‹¤ì œ ì´ë¯¸ì§€ ê²½ë¡œë¡œ ë³€ê²½í•´ì•¼ í•©ë‹ˆë‹¤! ğŸš¨ğŸš¨ğŸš¨
# QUERY_IMAGE_PATH = 'C:\dl_final\dl_test\training\Images\n02085936-Maltese_dog\n02085936_10719.jpg'
QUERY_IMAGE_PATH = 'training/Images/n02085936-Maltese_dog/n02085936_10719.jpg'

def setup_ap10k_model(config_file=AP10K_CONFIG_FILE, checkpoint_file=AP10K_CHECKPOINT_FILE):
    """
    AP-10K í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ëª¨ë¸ ë° ì‹œê°í™” ë„êµ¬ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
    """
    if not os.path.exists(config_file):
        raise FileNotFoundError(f"AP-10K ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_file}\n"
                                 "MMPose ì„¤ì¹˜ ê²½ë¡œì™€ AP10K_CONFIG_FILE ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    if not os.path.exists(checkpoint_file):
        raise FileNotFoundError(f"AP-10K ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_file}\n"
                                 "ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ íŒŒì¼ì„ AP10K_CHECKPOINT_FILE ê²½ë¡œì— ë‘ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")

    device = torch.device("cpu") 
    print(f"í‚¤í¬ì¸íŠ¸ ê²€ì¶œì— ì‚¬ìš©í•  ì¥ì¹˜: {device}")

    cfg = Config.fromfile(config_file)
    if cfg.model.get('test_cfg') is None:
        cfg.model.test_cfg = Config(dict(flip_test=False, post_process='default', shift_heatmap=True,
                                        modulate_whr_weight=0, target_type='heatmap'))

    model = init_model(cfg, checkpoint_file, device=device)
    
    # --- PoseLocalVisualizer ì´ˆê¸°í™” ---
    visualizer = PoseLocalVisualizer()
    visualizer.set_dataset_meta(model.dataset_meta) # ëª¨ë¸ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì‹œê°í™” ë„êµ¬ì— ì„¤ì •

    print("AP-10K í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ëª¨ë¸ ë° ì‹œê°í™” ë„êµ¬ ì¤€ë¹„ ì™„ë£Œ.")
    # dataset_infoëŠ” ë” ì´ìƒ ë°˜í™˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    return model, device, visualizer # ë°˜í™˜ ê°’ ë³€ê²½

def detect_and_visualize_keypoints(
    image_path: str, 
    ap10k_model, 
    device, 
    visualizer, 
    output_dir: str = 'output_keypoints'
):
    """
    ë‹¨ì¼ ì´ë¯¸ì§€ì—ì„œ í‚¤í¬ì¸íŠ¸ë¥¼ ê²€ì¶œí•˜ê³  ì‹œê°í™”í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.
    """
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    img = mmcv.imread(image_path, channel_order='rgb')
    if img is None:
        print(f"ì˜¤ë¥˜: ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}. íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return None, None # ì˜¤ë¥˜ ë°œìƒ ì‹œ None ë°˜í™˜

    img_height, img_width, _ = img.shape

    # MMPoseì˜ ê³ ìˆ˜ì¤€ ì¶”ë¡  API ì‚¬ìš©
    from mmpose.apis import inference_topdown
    
    # ì „ì²´ ì´ë¯¸ì§€ì— ëŒ€í•´ bounding box ìƒì„± (ë™ë¬¼ ì „ì²´ë¥¼ í¬í•¨)
    # bboxëŠ” [x1, y1, x2, y2] í˜•íƒœì˜ numpy ë°°ì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤
    bbox = np.array([[0, 0, img_width, img_height]], dtype=np.float32)
    
    # inference_topdownì„ ì‚¬ìš©í•˜ì—¬ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ
    pose_results = inference_topdown(ap10k_model, image_path, bbox)
    
    if not pose_results:
        print(f"ê²½ê³ : {image_path}ì—ì„œ í‚¤í¬ì¸íŠ¸ë¥¼ ê²€ì¶œí•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤. (ë™ë¬¼ì´ ì—†ê±°ë‚˜ ë„ˆë¬´ ì‘ì„ ìˆ˜ ìˆìŒ)")
        return None, None

    # ê³ ê¸‰ í‚¤í¬ì¸íŠ¸ ì‹œê°í™”
    vis_img = draw_advanced_keypoints(img.copy(), pose_results)
    
    base_name = os.path.basename(image_path)
    name_parts = os.path.splitext(base_name)
    output_filename = f"{name_parts[0]}_keypoints{name_parts[1]}"
    output_path = os.path.join(output_dir, output_filename)
    
    mmcv.imwrite(vis_img, output_path)
    print(f"í‚¤í¬ì¸íŠ¸ ì‹œê°í™” ì €ì¥: {output_path}")

    # ê²°ê³¼ë¥¼ ì´ì „ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    reconstructed_pose_results = []
    for pose_result in pose_results:
        if hasattr(pose_result, 'pred_instances'):
            keypoints_xy = pose_result.pred_instances.keypoints
            keypoint_scores = pose_result.pred_instances.keypoint_scores
            
            # í…ì„œì¸ ê²½ìš° numpyë¡œ ë³€í™˜
            if hasattr(keypoints_xy, 'cpu'):
                keypoints_xy = keypoints_xy.cpu().numpy()
            if hasattr(keypoint_scores, 'cpu'):
                keypoint_scores = keypoint_scores.cpu().numpy()
            
            for i in range(keypoints_xy.shape[0]): 
                kpt_xy = keypoints_xy[i] 
                kpt_score = keypoint_scores[i] 
                combined_kpts = np.hstack((kpt_xy, kpt_score[:, np.newaxis]))
                reconstructed_pose_results.append({'keypoints': combined_kpts})
        else:
            # ì´ì „ í˜•ì‹ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            reconstructed_pose_results.append(pose_result)

    return output_path, reconstructed_pose_results 

def calculate_keypoint_similarity(pose_results1, pose_results2, image_size=SIMCLR_IMAGE_SIZE):
    """
    ë‘ í‚¤í¬ì¸íŠ¸ ê²°ê³¼ ê°„ì˜ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    (ê°„ë‹¨í•œ ì˜ˆì‹œ: í‚¤í¬ì¸íŠ¸ ìœ„ì¹˜ì˜ L2 ê±°ë¦¬ ê¸°ë°˜)
    """
    if not pose_results1 or not pose_results2:
        return 0.0 

    kpts1 = pose_results1[0]['keypoints'] 
    kpts2 = pose_results2[0]['keypoints']

    min_kpts = min(len(kpts1), len(kpts2)) 
    kpts1_xy = kpts1[:min_kpts, :2] 
    kpts2_xy = kpts2[:min_kpts, :2] 
    
    kpts1_xy_t = torch.from_numpy(kpts1_xy).float()
    kpts2_xy_t = torch.from_numpy(kpts2_xy).float()

    distance = torch.mean(torch.norm(kpts1_xy_t - kpts2_xy_t, dim=1)) / image_size

    similarity = float(max(0.0, 1.0 - distance.item())) 
    return similarity

def draw_advanced_keypoints(img, pose_results):
    """
    ê³ ê¸‰ í‚¤í¬ì¸íŠ¸ ì‹œê°í™” - ê³¨ê²© êµ¬ì¡°ë¥¼ ì„ ìœ¼ë¡œ ì—°ê²°í•˜ê³  ì˜ˆìœ ìƒ‰ìƒ ì ìš©
    """
    # AP10K ë°ì´í„°ì…‹ì˜ í‚¤í¬ì¸íŠ¸ ìˆœì„œ (17ê°œ í‚¤í¬ì¸íŠ¸)
    # 0: ì½”ë, 1: ì¢Œì•ˆ, 2: ìš°ì•ˆ, 3: ì¢Œê·€, 4: ìš°ê·€, 5: ëª©, 6: ì¢Œì–´ê¹¨, 7: ìš°ì–´ê¹¨,
    # 8: ì¢ŒíŒ”ê¿ˆì¹˜, 9: ìš°íŒ”ê¿ˆì¹˜, 10: ì¢Œë°œëª©, 11: ìš°ë°œëª©, 12: ì¢Œì—‰ë©ì´, 13: ìš°ì—‰ë©ì´,
    # 14: ì¢Œë¬´ë¦, 15: ìš°ë¬´ë¦, 16: ê¼¬ë¦¬ë
    
    # ê³¨ê²© ì—°ê²° ì •ë³´ (ì—°ê²°í•  í‚¤í¬ì¸íŠ¸ ìŒë“¤)
    skeleton_connections = [
        # ë¨¸ë¦¬ ë¶€ë¶„
        (1, 2),   # ì¢Œì•ˆ - ìš°ì•ˆ
        (1, 3),   # ì¢Œì•ˆ - ì¢Œê·€  
        (2, 4),   # ìš°ì•ˆ - ìš°ê·€
        (0, 1),   # ì½”ë - ì¢Œì•ˆ
        (0, 2),   # ì½”ë - ìš°ì•ˆ
        (5, 1),   # ëª© - ì¢Œì•ˆ
        (5, 2),   # ëª© - ìš°ì•ˆ
        
        # ëª¸í†µ ë¶€ë¶„
        (5, 6),   # ëª© - ì¢Œì–´ê¹¨
        (5, 7),   # ëª© - ìš°ì–´ê¹¨
        (6, 12),  # ì¢Œì–´ê¹¨ - ì¢Œì—‰ë©ì´
        (7, 13),  # ìš°ì–´ê¹¨ - ìš°ì—‰ë©ì´
        (12, 13), # ì¢Œì—‰ë©ì´ - ìš°ì—‰ë©ì´
        
        # ì•ë‹¤ë¦¬
        (6, 8),   # ì¢Œì–´ê¹¨ - ì¢ŒíŒ”ê¿ˆì¹˜
        (8, 10),  # ì¢ŒíŒ”ê¿ˆì¹˜ - ì¢Œë°œëª©
        (7, 9),   # ìš°ì–´ê¹¨ - ìš°íŒ”ê¿ˆì¹˜
        (9, 11),  # ìš°íŒ”ê¿ˆì¹˜ - ìš°ë°œëª©
        
        # ë’·ë‹¤ë¦¬
        (12, 14), # ì¢Œì—‰ë©ì´ - ì¢Œë¬´ë¦
        (14, 16), # ì¢Œë¬´ë¦ - ê¼¬ë¦¬ë (ì„ì‹œ)
        (13, 15), # ìš°ì—‰ë©ì´ - ìš°ë¬´ë¦
        (15, 16), # ìš°ë¬´ë¦ - ê¼¬ë¦¬ë (ì„ì‹œ)
    ]
    
    # ìƒ‰ìƒ ì •ì˜ (BGR í˜•ì‹)
    colors = {
        'keypoint': (0, 255, 255),      # ë…¸ë€ìƒ‰ í‚¤í¬ì¸íŠ¸
        'head': (0, 0, 255),            # ğŸ”´ ë¹¨ê°„ìƒ‰ - ë¨¸ë¦¬ ë¶€ë¶„
        'body': (0, 255, 0),            # ğŸŸ¢ ì´ˆë¡ìƒ‰ - ëª¸í†µ
        'front_legs': (0, 255, 255),    # ğŸŸ¡ ë…¸ë€ìƒ‰ - ì•ë‹¤ë¦¬
        'back_legs': (0, 165, 255),     # ğŸŸ  ì£¼í™©ìƒ‰ - ë’·ë‹¤ë¦¬
    }
    
    for pose_result in pose_results:
        if hasattr(pose_result, 'pred_instances'):
            keypoints = pose_result.pred_instances.keypoints
            scores = pose_result.pred_instances.keypoint_scores
            
            # í…ì„œì¸ ê²½ìš° numpyë¡œ ë³€í™˜
            if hasattr(keypoints, 'cpu'):
                keypoints = keypoints.cpu().numpy()
            if hasattr(scores, 'cpu'):
                scores = scores.cpu().numpy()
        else:
            keypoints = pose_result['keypoints']
            scores = pose_result.get('keypoint_scores', keypoints[..., 2:3] if keypoints.shape[-1] > 2 else None)
        
        # ì²« ë²ˆì§¸ ì¸ìŠ¤í„´ìŠ¤ì˜ í‚¤í¬ì¸íŠ¸ë§Œ ì‹œê°í™”
        if len(keypoints) > 0:
            kpts = keypoints[0]  # (17, 2) ë˜ëŠ” (17, 3)
            kpt_scores = scores[0] if scores is not None else np.ones(len(kpts))
            
            confidence_threshold = 0.3
            
            # íˆ¬ëª…ë„ ì ìš©ì„ ìœ„í•œ ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€ ìƒì„±
            overlay = img.copy()
            
            # ë¨¼ì € ê³¨ê²© ì„  ê·¸ë¦¬ê¸° (ì˜¤ë²„ë ˆì´ì—)
            for connection in skeleton_connections:
                pt1_idx, pt2_idx = connection
                if (pt1_idx < len(kpts) and pt2_idx < len(kpts) and 
                    kpt_scores[pt1_idx] > confidence_threshold and 
                    kpt_scores[pt2_idx] > confidence_threshold):
                    
                    pt1 = (int(kpts[pt1_idx][0]), int(kpts[pt1_idx][1]))
                    pt2 = (int(kpts[pt2_idx][0]), int(kpts[pt2_idx][1]))
                    
                    # ì—°ê²°ì„  ìƒ‰ìƒ ê²°ì •
                    if connection in [(1, 2), (1, 3), (2, 4), (0, 1), (0, 2), (5, 1), (5, 2)]:
                        line_color = colors['head']
                    elif connection in [(5, 6), (5, 7), (6, 12), (7, 13), (12, 13)]:
                        line_color = colors['body']
                    elif connection in [(6, 8), (8, 10), (7, 9), (9, 11)]:
                        line_color = colors['front_legs']
                    else:
                        line_color = colors['back_legs']
                    
                    cv2.line(overlay, pt1, pt2, line_color, 2)  # ì„  ë‘ê»˜ ê°ì†Œ
            
            # ê³¨ê²©ì„  30% íˆ¬ëª…ë„ ì ìš© (alpha=0.3)
            alpha = 0.3
            cv2.addWeighted(overlay, alpha, img, 1 - alpha, 0, img)
            
            # í‚¤í¬ì¸íŠ¸ ì  ê·¸ë¦¬ê¸° (íˆ¬ëª…ë„ ì ìš©)
            kpt_overlay = img.copy()
            for i, (kpt, score) in enumerate(zip(kpts, kpt_scores)):
                if score > confidence_threshold:
                    pt = (int(kpt[0]), int(kpt[1]))
                    
                    # í‚¤í¬ì¸íŠ¸ë³„ ìƒ‰ìƒ (ìƒˆë¡œìš´ ìƒ‰ìƒ ì²´ê³„)
                    if i in [0, 1, 2, 3, 4]:  # ë¨¸ë¦¬ ë¶€ë¶„
                        kpt_color = (0, 0, 255)      # ë¹¨ê°„ìƒ‰
                    elif i == 5:  # ëª©
                        kpt_color = (0, 255, 0)      # ì´ˆë¡ìƒ‰
                    elif i in [6, 7, 8, 9, 10, 11]:  # ì•ë‹¤ë¦¬
                        kpt_color = (0, 255, 255)    # ë…¸ë€ìƒ‰
                    else:  # ë’·ë‹¤ë¦¬, ê¼¬ë¦¬
                        kpt_color = (0, 165, 255)    # ì£¼í™©ìƒ‰
                    
                    # í‚¤í¬ì¸íŠ¸ ì› ê·¸ë¦¬ê¸° (ì˜¤ë²„ë ˆì´ì—)
                    cv2.circle(kpt_overlay, pt, 5, kpt_color, -1)
                    cv2.circle(kpt_overlay, pt, 6, (255, 255, 255), 2)  # í°ìƒ‰ í…Œë‘ë¦¬
                    
                    # í‚¤í¬ì¸íŠ¸ ë²ˆí˜¸ í‘œì‹œ (ì˜¤ë²„ë ˆì´ì—)
                    cv2.putText(kpt_overlay, str(i), (pt[0] + 8, pt[1] - 8), 
                              cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
            
            # í‚¤í¬ì¸íŠ¸ ì  50% íˆ¬ëª…ë„ ì ìš©
            kpt_alpha = 0.5
            cv2.addWeighted(kpt_overlay, kpt_alpha, img, 1 - kpt_alpha, 0, img)
    
    return img

if __name__ == "__main__":
    # --- AP-10K ëª¨ë¸ ë° ì‹œê°í™” ë„êµ¬ ë¡œë“œ ---
    # setup_ap10k_modelì˜ ë°˜í™˜ ê°’ì— dataset_infoê°€ ì—†ìŠµë‹ˆë‹¤.
    ap10k_model, ap10k_device, visualizer = setup_ap10k_model()

    # --- SimCLR ìœ ì‚¬ ê°•ì•„ì§€ ê²€ìƒ‰ ---
    query_image_path = QUERY_IMAGE_PATH 

    if not os.path.exists(query_image_path):
        print(f"ì˜¤ë¥˜: ì¿¼ë¦¬ ì´ë¯¸ì§€ '{query_image_path}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ ìˆ˜ì •í•˜ê±°ë‚˜ íŒŒì¼ì„ ì¤€ë¹„í•´ì£¼ì„¸ìš”.")
    else:
        print(f"\nì¿¼ë¦¬ ì´ë¯¸ì§€: {query_image_path}")
        print("SimCLR ê¸°ë°˜ ìœ ì‚¬ ê°•ì•„ì§€ ê²€ìƒ‰ ì‹œì‘...")
        top_similar_dogs_simclr = search_similar_dogs(
            query_image_path=query_image_path, 
            top_k=5,
            model_path=SIMCLR_MODEL_PATH,
            out_dim=SIMCLR_OUT_DIM,
            image_size=SIMCLR_IMAGE_SIZE,
            db_features_file=DB_FEATURES_FILE,
            db_image_paths_file=DB_IMAGE_PATHS_FILE
        )
        
        print("\n--- SimCLR ê¸°ë°˜ ê°€ì¥ ìœ ì‚¬í•œ ê°•ì•„ì§€ ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ 5ê°œ) ---")
        # ì¿¼ë¦¬ ì´ë¯¸ì§€ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ë° ì‹œê°í™”
        print(f"\nì¿¼ë¦¬ ì´ë¯¸ì§€ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ë° ì‹œê°í™”: {query_image_path}")
        # detect_and_visualize_keypoints í˜¸ì¶œ ì‹œ dataset_info ì¸ì ì œê±°
        query_kp_output_path, query_pose_results = detect_and_visualize_keypoints(
            query_image_path, ap10k_model, ap10k_device, visualizer
        )

        final_results = []

        for i, (simclr_score, db_img_path) in enumerate(top_similar_dogs_simclr):
            print(f"\n--- ìœ ì‚¬ ê°•ì•„ì§€ {i+1}: {db_img_path} (SimCLR ìœ ì‚¬ë„: {simclr_score:.4f}) ---")
            
            # DB ì´ë¯¸ì§€ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ë° ì‹œê°í™”
            # detect_and_visualize_keypoints í˜¸ì¶œ ì‹œ dataset_info ì¸ì ì œê±°
            db_kp_output_path, db_pose_results = detect_and_visualize_keypoints(
                db_img_path, ap10k_model, ap10k_device, visualizer
            )

            keypoint_similarity = 0.0
            if query_pose_results and db_pose_results:
                keypoint_similarity = calculate_keypoint_similarity(query_pose_results, db_pose_results, SIMCLR_IMAGE_SIZE)
                print(f"í‚¤í¬ì¸íŠ¸ ìœ ì‚¬ë„: {keypoint_similarity:.4f}")
            else:
                print("í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ê²°ê³¼ ì—†ìŒ.")
            
            final_score = (simclr_score * 0.7) + (keypoint_similarity * 0.3) 
            final_results.append((final_score, db_img_path, simclr_score, keypoint_similarity))
            
            print(f"ìµœì¢… ë³µí•© ìœ ì‚¬ë„: {final_score:.4f}")

        final_results.sort(key=lambda x: x[0], reverse=True)

        print("\n--- ìµœì¢… ë³µí•© ìœ ì‚¬ë„ ê¸°ì¤€ ê²€ìƒ‰ ê²°ê³¼ ---")
        for i, (final_score, path, simclr_s, kp_s) in enumerate(final_results):
            print(f"{i+1}. ìµœì¢… ìœ ì‚¬ë„: {final_score:.4f}, SimCLR: {simclr_s:.4f}, KP: {kp_s:.4f}, ì´ë¯¸ì§€ ê²½ë¡œ: {path}")