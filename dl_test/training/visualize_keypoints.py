import re
def normalize_filename(filename):
    # íŒŒì¼ëª…ì—ì„œ ì—°ì†ëœ íŠ¹ìˆ˜ë¬¸ì, ê³µë°±, ê´„í˜¸ ë“±ì€ ëª¨ë‘ ë‹¨ì¼ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ì¹˜í™˜
    name, ext = os.path.splitext(filename)
    # 1. ê´„í˜¸, ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ëª¨ë‘ _ë¡œ ì¹˜í™˜
    name = re.sub(r'[^A-Za-z0-9._-]+', '_', name)
    # 2. ì•ë’¤ _ ì œê±°
    name = name.strip('_')
    # 3. ì—°ì†ëœ _ëŠ” í•˜ë‚˜ë¡œ ì¶•ì†Œ
    name = re.sub(r'_+', '_', name)
    return name + ext
import torch
import numpy as np
import os
from PIL import Image
import cv2 # OpenCVëŠ” ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ì‹œê°í™”ì— ìœ ìš©í•©ë‹ˆë‹¤.
import mmcv
import io
import requests

# --- MMPose 1.3.2+ ë²„ì „ì— ë§ëŠ” ì„í¬íŠ¸ ---
from mmpose.apis import init_model 
from mmpose.visualization import PoseLocalVisualizer 
# from mmpose.datasets import DatasetInfo # <-- ì´ ì¤„ì€ ì‚­ì œí•©ë‹ˆë‹¤!
from mmengine import Config

# --- MMPose 1.3.2+ ë²„ì „ìš© êµ¬ì¡°ì²´ ì„í¬íŠ¸ ---
from mmpose.structures import PoseDataSample
from mmengine.structures import InstanceData # bboxë¥¼ ë‹´ê¸° ìœ„í•´ í•„ìš”

# --- SimCLR íŠ¹ì§• ê²€ìƒ‰ í•¨ìˆ˜ëŠ” í•„ìš”ì‹œ ë™ì ìœ¼ë¡œ ì„í¬íŠ¸ ---
# ìˆœí™˜ ì°¸ì¡° ë°©ì§€ë¥¼ ìœ„í•´ ì£¼ì„ ì²˜ë¦¬
# search_similar_dogs í•¨ìˆ˜ëŠ” backendì—ì„œ ì§ì ‘ ì„í¬íŠ¸í•˜ì—¬ ì‚¬ìš© 

# --- AP-10K ëª¨ë¸ ë° ì„¤ì • ê²½ë¡œ ---
MMPose_ROOT = 'C:/dl_final/dl_fianl/mm_pose/mmpose' # ê²½ë¡œ ìˆ˜ì •: dl_fianlë¡œ ë³€ê²½

# ì„¤ì • íŒŒì¼ ê²½ë¡œ 
AP10K_CONFIG_FILE = os.path.join(MMPose_ROOT, 'configs', 'animal_2d_keypoint', 
                                 'topdown_heatmap', 'ap10k', 'td-hm_hrnet-w32_8xb64-210e_ap10k-256x256.py') 

# ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ 
AP10K_CHECKPOINT_FILE = os.path.join(MMPose_ROOT, 'checkpoints', 'hrnet_w32_ap10k_256x256-18aac840_20211029.pth') 

# --- SimCLR ê´€ë ¨ ì„¤ì • (ì´ì „ íŒŒì¼ë“¤ì—ì„œ ê°€ì ¸ì˜´) ---

# ê°•ì•„ì§€ keypoints ê¸°ë°˜ bounding box ì¶”ì¶œ util í•¨ìˆ˜ (auto_pet_register.pyì—ì„œ importìš©)
def get_dog_bbox_from_keypoints(image_path, ap10k_model, device, min_score=0.3):
    """
    ì´ë¯¸ì§€ì—ì„œ ê°•ì•„ì§€ keypointsë¥¼ ê²€ì¶œí•˜ì—¬ bounding box(x1, y1, x2, y2) ë°˜í™˜.
    keypoints ì‹ ë¢°ë„ min_score ì´ìƒë§Œ ì‚¬ìš©. ì‹¤íŒ¨ ì‹œ None ë°˜í™˜.
    """
    from PIL import Image
    import numpy as np
    import cv2
    import io, requests, os
    def pad_to_square(img, fill_color=(255,255,255)):
        w, h = img.size
        if w == h:
            return img
        size = max(w, h)
        new_img = Image.new('RGB', (size, size), fill_color)
        new_img.paste(img, ((size - w) // 2, (size - h) // 2))
        return new_img

    if image_path.startswith('http://') or image_path.startswith('https://'):
        try:
            response = requests.get(image_path, timeout=10)
            response.raise_for_status()
            img_source = Image.open(io.BytesIO(response.content)).convert('RGB')
        except Exception as e:
            print(f"[bbox] URL ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {image_path}\n{e}")
            return None
    else:
        if not os.path.exists(image_path):
            print(f"[bbox] íŒŒì¼ ì—†ìŒ: {image_path}")
            return None
        img_source = Image.open(image_path).convert('RGB')

    img_source = pad_to_square(img_source)
    img_resized = img_source.resize((256, 256), Image.BILINEAR)
    img_rgb = np.array(img_resized)
    img_height, img_width = img_rgb.shape[:2]

    from mmpose.apis import inference_topdown
    bbox = np.array([[0, 0, img_width, img_height]], dtype=np.float32)
    pose_results = inference_topdown(ap10k_model, img_rgb, bbox)
    if not pose_results:
        return None
    # keypoints ì¶”ì¶œ
    if hasattr(pose_results[0], 'pred_instances'):
        keypoints = pose_results[0].pred_instances.keypoints[0]
        scores = pose_results[0].pred_instances.keypoint_scores[0]
        kpts_xy = keypoints[scores > min_score]
    else:
        kpts = pose_results[0]['keypoints']
        if kpts.shape[1] == 3:
            scores = kpts[:, 2]
            kpts_xy = kpts[scores > min_score, :2]
        else:
            kpts_xy = kpts[:, :2]
    if len(kpts_xy) == 0:
        return None
    x_min, y_min = kpts_xy.min(axis=0)
    x_max, y_max = kpts_xy.max(axis=0)
    # ì—¬ìœ  padding (10%)
    pad_x = int((x_max - x_min) * 0.1)
    pad_y = int((y_max - y_min) * 0.1)
    x1 = max(int(x_min) - pad_x, 0)
    y1 = max(int(y_min) - pad_y, 0)
    x2 = min(int(x_max) + pad_x, img_width)
    y2 = min(int(y_max) + pad_y, img_height)
    return (x1, y1, x2, y2)
SIMCLR_MODEL_PATH = 'models/simclr_vit_dog_model_finetuned_v1.pth'
SIMCLR_OUT_DIM = 128
SIMCLR_IMAGE_SIZE = 224
DB_FEATURES_FILE = 'db_features.npy' 
DB_IMAGE_PATHS_FILE = 'db_image_paths.npy'

# ì¿¼ë¦¬ ì´ë¯¸ì§€ ê²½ë¡œ (í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì„¤ì •, ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œëŠ” ì‚¬ìš©ì ì…ë ¥ ë°›ìŒ)
# ğŸš¨ğŸš¨ğŸš¨ ì´ ê²½ë¡œë„ ë³¸ì¸ì˜ ì‹¤ì œ ì´ë¯¸ì§€ ê²½ë¡œë¡œ ë³€ê²½í•´ì•¼ í•©ë‹ˆë‹¤! ğŸš¨ğŸš¨ğŸš¨
# QUERY_IMAGE_PATH = 'C:\dl_final\dl_test\training\Images\n02085936-Maltese_dog\n02085936_10719.jpg'
QUERY_IMAGE_PATH = 'training/Images/n02085936-Maltese_dog/n02085936_10719.jpg'

def setup_ap10k_model(config_file=AP10K_CONFIG_FILE, checkpoint_file=AP10K_CHECKPOINT_FILE):
    """
    AP-10K í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ëª¨ë¸ ë° ì‹œê°í™” ë„êµ¬ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
    """
    if not os.path.exists(config_file):
        raise FileNotFoundError(f"AP-10K ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_file}\n"
                                 "MMPose ì„¤ì¹˜ ê²½ë¡œì™€ AP10K_CONFIG_FILE ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    if not os.path.exists(checkpoint_file):
        raise FileNotFoundError(f"AP-10K ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_file}\n"
                                 "ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ íŒŒì¼ì„ AP10K_CHECKPOINT_FILE ê²½ë¡œì— ë‘ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")

    device = torch.device("cpu") 
    print(f"í‚¤í¬ì¸íŠ¸ ê²€ì¶œì— ì‚¬ìš©í•  ì¥ì¹˜: {device}")

    cfg = Config.fromfile(config_file)
    if cfg.model.get('test_cfg') is None:
        cfg.model.test_cfg = Config(dict(flip_test=False, post_process='default', shift_heatmap=True,
                                        modulate_whr_weight=0, target_type='heatmap'))

    model = init_model(cfg, checkpoint_file, device=device)
    
    # --- PoseLocalVisualizer ì´ˆê¸°í™” ---
    visualizer = PoseLocalVisualizer()
    visualizer.set_dataset_meta(model.dataset_meta) # ëª¨ë¸ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì‹œê°í™” ë„êµ¬ì— ì„¤ì •

    print("AP-10K í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ëª¨ë¸ ë° ì‹œê°í™” ë„êµ¬ ì¤€ë¹„ ì™„ë£Œ.")
    # dataset_infoëŠ” ë” ì´ìƒ ë°˜í™˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    return model, device, visualizer # ë°˜í™˜ ê°’ ë³€ê²½

def detect_and_visualize_keypoints(
    image_path: str, 
    ap10k_model, 
    device, 
    visualizer, 
    output_dir: str = None,
    output_path: str = None
):
    """
    ë‹¨ì¼ ì´ë¯¸ì§€ì—ì„œ í‚¤í¬ì¸íŠ¸ë¥¼ ê²€ì¶œí•˜ê³  ì‹œê°í™”í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.
    image_pathê°€ http/httpsë¡œ ì‹œì‘í•˜ë©´ URLì—ì„œ ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
    """
    # ëª¨ë“  output_keypoints ì €ì¥/ì„œë¹™ ê²½ë¡œë¥¼ C:/dl_final/dl_fianl/output_keypointsë¡œ ê°•ì œ í†µì¼
    fixed_output_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', 'output_keypoints'))
    print(f"[DEBUG] __file__ ìœ„ì¹˜: {os.path.abspath(__file__)}")
    print(f"[DEBUG] fixed_output_dir (dl_fianl/output_keypoints): {fixed_output_dir}")
    if not os.path.exists(fixed_output_dir):
        os.makedirs(fixed_output_dir)
    output_dir = fixed_output_dir
    # FastAPI StaticFiles ë§ˆìš´íŠ¸ ê²½ë¡œë„ ë°˜ë“œì‹œ ë™ì¼í•˜ê²Œ ë§ì¶°ì•¼ í•¨
    print(f"[INFO] ëª¨ë“  output_keypoints ì €ì¥/ì„œë¹™ ê²½ë¡œë¥¼ C:/dl_final/dl_fianl/output_keypointsë¡œ ê°•ì œ í†µì¼í•©ë‹ˆë‹¤.")

    # --- ì´ë¯¸ì§€ ë¡œë”© (ë¡œì»¬/URL ëª¨ë‘ ì§€ì›) ë° 256x256 ë¦¬ì‚¬ì´ì¦ˆ ---
    img_rgb = None
    img_source = None
    def pad_to_square(img, fill_color=(255,255,255)):
        w, h = img.size
        if w == h:
            return img
        size = max(w, h)
        new_img = Image.new('RGB', (size, size), fill_color)
        new_img.paste(img, ((size - w) // 2, (size - h) // 2))
        return new_img

    if image_path.startswith('http://') or image_path.startswith('https://'):
        try:
            response = requests.get(image_path, timeout=10)
            response.raise_for_status()
            img_source = Image.open(io.BytesIO(response.content)).convert('RGB')
        except Exception as e:
            print(f"ì˜¤ë¥˜: URLì—ì„œ ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}\n{e}")
            return None, None
    else:
        if not os.path.exists(image_path):
            print(f"ì˜¤ë¥˜: ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}. íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
            return None, None
        img_source = Image.open(image_path).convert('RGB')

    # ì •ì‚¬ê°í˜• íŒ¨ë”© í›„ 256x256 ë¦¬ì‚¬ì´ì¦ˆ ë° ë””ë²„ê·¸ ì´ë¯¸ì§€ ì €ì¥
    img_source = pad_to_square(img_source)
    # ë””ë²„ê·¸: íŒ¨ë”©ëœ ì´ë¯¸ì§€ ì €ì¥
    debug_pad_path = os.path.join(output_dir, f"{os.path.splitext(os.path.basename(image_path))[0]}_padded.jpg")
    img_source.save(debug_pad_path)
    print(f"[DEBUG] íŒ¨ë”© ì´ë¯¸ì§€ ì €ì¥: {debug_pad_path}")

    img_resized = img_source.resize((256, 256), Image.BILINEAR)
    # ë””ë²„ê·¸: ë¦¬ì‚¬ì´ì¦ˆëœ ì´ë¯¸ì§€ ì €ì¥
    debug_resize_path = os.path.join(output_dir, f"{os.path.splitext(os.path.basename(image_path))[0]}_resized256.jpg")
    Image.fromarray(np.array(img_resized)).save(debug_resize_path)
    print(f"[DEBUG] 256x256 ë¦¬ì‚¬ì´ì¦ˆ ì´ë¯¸ì§€ ì €ì¥: {debug_resize_path}")

    img_rgb = np.array(img_resized)
    print(f"[DEBUG] img_rgb shape: {img_rgb.shape}, dtype: {img_rgb.dtype}, min: {img_rgb.min()}, max: {img_rgb.max()}")

    # --- ë””ë²„ê¹…: ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ shape, dtype, min/max ê°’ ì¶œë ¥ ë° ì €ì¥ ---
    print(f"[DEBUG] Preprocessed image shape: {img_rgb.shape}, dtype: {img_rgb.dtype}, min: {img_rgb.min()}, max: {img_rgb.max()}")
    # ì¤‘ê°„ ì´ë¯¸ì§€ ì €ì¥ (ë””ë²„ê¹…ìš©)
    debug_padded_path = os.path.join(output_dir, 'debug_padded_' + os.path.basename(image_path))
    debug_resized_path = os.path.join(output_dir, 'debug_resized_' + os.path.basename(image_path))
    try:
        img_source.save(debug_padded_path)
        Image.fromarray(img_rgb).save(debug_resized_path)
        print(f"[DEBUG] Saved padded image: {debug_padded_path}")
        print(f"[DEBUG] Saved resized image: {debug_resized_path}")
    except Exception as e:
        print(f"[DEBUG] Failed to save debug images: {e}")
    img_height, img_width = img_rgb.shape[:2]
    # ì›ë³¸ í¬ê¸° ì •ë³´ë„ pose_resultsì— í¬í•¨ (ì•„ë˜ì—ì„œ img_width, img_heightë¡œ ì €ì¥)

    # MMPoseì˜ ê³ ìˆ˜ì¤€ ì¶”ë¡  API ì‚¬ìš©
    from mmpose.apis import inference_topdown
    bbox = np.array([[0, 0, img_width, img_height]], dtype=np.float32)

    # inference_topdownì€ numpy arrayë„ ì…ë ¥ ê°€ëŠ¥

    pose_results = inference_topdown(ap10k_model, img_rgb, bbox)

    # --- ë””ë²„ê¹…: pose_resultsê°€ ë¹„ì—ˆì„ ë•Œ shape, dtype, íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ì¶œë ¥ ---
    if not pose_results:
        print(f"[DEBUG] pose_results is empty for {image_path}")
        print(f"[DEBUG] Preprocessed image shape: {img_rgb.shape}, dtype: {img_rgb.dtype}, min: {img_rgb.min()}, max: {img_rgb.max()}")
        print(f"[DEBUG] Does padded image exist? {os.path.exists(debug_padded_path)}")
        print(f"[DEBUG] Does resized image exist? {os.path.exists(debug_resized_path)}")
        print(f"ê²½ê³ : {image_path}ì—ì„œ í‚¤í¬ì¸íŠ¸ë¥¼ ê²€ì¶œí•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤. (ë™ë¬¼ì´ ì—†ê±°ë‚˜ ë„ˆë¬´ ì‘ì„ ìˆ˜ ìˆìŒ)")
        return None, None

    vis_img_rgb = draw_advanced_keypoints_rgb(img_rgb.copy(), pose_results)

    # íŒŒì¼ëª… ìƒì„± (URLì´ë©´ íŒŒì¼ëª…ë§Œ ì¶”ì¶œ, í™•ì¥ìëŠ” í•­ìƒ .jpgë¡œ ê°•ì œ)
    if output_path is not None:
        # ì™¸ë¶€ì—ì„œ ê²½ë¡œë¥¼ ì§€ì •í•œ ê²½ìš° í•´ë‹¹ ê²½ë¡œì— ì €ì¥ (ì •ê·œí™” ì ìš©)
        output_path = os.path.abspath(output_path)
        output_path = os.path.join(os.path.dirname(output_path), normalize_filename(os.path.basename(output_path)))
    else:
        base_name = os.path.basename(image_path)
        if not base_name:
            base_name = 'remote_image'
        base_name = normalize_filename(base_name)
        name_parts = os.path.splitext(base_name)
        output_filename = f"{name_parts[0]}_keypoints.jpg"  # í•­ìƒ jpgë¡œ ì €ì¥
        output_path = os.path.join(output_dir, output_filename)

    pil_img = Image.fromarray(vis_img_rgb)
    pil_img.save(output_path, format='JPEG')
    print(f"í‚¤í¬ì¸íŠ¸ ì‹œê°í™” ì €ì¥: {output_path}")

    # ê²°ê³¼ë¥¼ ì´ì „ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (width, height ì •ë³´ í¬í•¨)
    reconstructed_pose_results = []
    for pose_result in pose_results:
        if hasattr(pose_result, 'pred_instances'):
            keypoints_xy = pose_result.pred_instances.keypoints
            keypoint_scores = pose_result.pred_instances.keypoint_scores
            # í…ì„œì¸ ê²½ìš° numpyë¡œ ë³€í™˜
            if hasattr(keypoints_xy, 'cpu'):
                keypoints_xy = keypoints_xy.cpu().numpy()
            if hasattr(keypoint_scores, 'cpu'):
                keypoint_scores = keypoint_scores.cpu().numpy()
            for i in range(keypoints_xy.shape[0]):
                kpt_xy = keypoints_xy[i]
                kpt_score = keypoint_scores[i]
                combined_kpts = np.hstack((kpt_xy, kpt_score[:, np.newaxis]))
                reconstructed_pose_results.append({'keypoints': combined_kpts, 'img_width': img_width, 'img_height': img_height})
        else:
            # ì´ì „ í˜•ì‹ ê·¸ëŒ€ë¡œ ì‚¬ìš©, width/height ì¶”ê°€
            pose_dict = dict(pose_result)
            pose_dict['img_width'] = img_width
            pose_dict['img_height'] = img_height
            reconstructed_pose_results.append(pose_dict)

    # pose_resultsê°€ ë¹„ì–´ìˆìœ¼ë©´ None ë°˜í™˜ (ì–¸íŒ© ì—ëŸ¬ ë°©ì§€)
    if not reconstructed_pose_results:
        return None, None
    return output_path, reconstructed_pose_results

def calculate_keypoint_similarity(pose_results1, pose_results2, image_size=SIMCLR_IMAGE_SIZE):
    """
    ë‘ í‚¤í¬ì¸íŠ¸ ê²°ê³¼ ê°„ì˜ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    (ê°„ë‹¨í•œ ì˜ˆì‹œ: í‚¤í¬ì¸íŠ¸ ìœ„ì¹˜ì˜ L2 ê±°ë¦¬ ê¸°ë°˜)
    """
    if not pose_results1 or not pose_results2:
        print("[DEBUG] calculate_keypoint_similarity: pose_results1 or pose_results2 is empty")
        return 0.0

    kpts1 = pose_results1[0]['keypoints']
    kpts2 = pose_results2[0]['keypoints']

    min_kpts = min(len(kpts1), len(kpts2))
    kpts1_xy = kpts1[:min_kpts, :2]
    kpts2_xy = kpts2[:min_kpts, :2]

    # ê° ì´ë¯¸ì§€ì˜ width, heightë¡œ ì •ê·œí™”
    w1 = pose_results1[0].get('img_width', image_size)
    h1 = pose_results1[0].get('img_height', image_size)
    w2 = pose_results2[0].get('img_width', image_size)
    h2 = pose_results2[0].get('img_height', image_size)

    # (x, y) ê°ê° ì •ê·œí™”
    kpts1_xy_norm = np.zeros_like(kpts1_xy)
    kpts2_xy_norm = np.zeros_like(kpts2_xy)
    kpts1_xy_norm[:, 0] = kpts1_xy[:, 0] / w1
    kpts1_xy_norm[:, 1] = kpts1_xy[:, 1] / h1
    kpts2_xy_norm[:, 0] = kpts2_xy[:, 0] / w2
    kpts2_xy_norm[:, 1] = kpts2_xy[:, 1] / h2

    # print(f"[DEBUG] kpts1_xy_norm: {kpts1_xy_norm}")
    # print(f"[DEBUG] kpts2_xy_norm: {kpts2_xy_norm}")  # [ìë™ ì£¼ì„ì²˜ë¦¬] kpts2_xy_norm ë””ë²„ê·¸ ì¶œë ¥

    # NaN/Inf ì²´í¬
    if np.any(np.isnan(kpts1_xy_norm)) or np.any(np.isnan(kpts2_xy_norm)):
        print("[DEBUG] NaN detected in normalized keypoints!")
        return 0.0
    if np.any(np.isinf(kpts1_xy_norm)) or np.any(np.isinf(kpts2_xy_norm)):
        print("[DEBUG] Inf detected in normalized keypoints!")
        return 0.0

    kpts1_xy_t = torch.from_numpy(kpts1_xy_norm).float()
    kpts2_xy_t = torch.from_numpy(kpts2_xy_norm).float()

    distance = torch.mean(torch.norm(kpts1_xy_t - kpts2_xy_t, dim=1))
    print(f"[DEBUG] keypoint L2 distance: {distance.item()}")

    # distanceê°€ 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬ë„ 1, distanceê°€ ì»¤ì§ˆìˆ˜ë¡ ìœ ì‚¬ë„ 0ì— ê°€ê¹Œì›Œì§
    similarity = float(1.0 / (1.0 + distance.item()))
    print(f"[DEBUG] keypoint similarity (1/(1+d)): {similarity}")
    return similarity

def draw_advanced_keypoints_rgb(img_rgb, pose_results):
    """
    RGB ì´ë¯¸ì§€ì— í‚¤í¬ì¸íŠ¸ì™€ ê³¨ê²©ì„ ê·¸ë¦¬ëŠ” í•¨ìˆ˜ (íˆ¬ëª…ë„ 30% ì ìš©)
    """
    if not pose_results:
        return img_rgb
    
    # AP-10K 17ê°œ í‚¤í¬ì¸íŠ¸ ê³¨ê²© ì—°ê²° ì •ì˜
    skeleton_connections = [
        # ë¨¸ë¦¬
        (1, 2),   # ì¢Œê·€ - ìš°ê·€
        (1, 3),   # ì¢Œê·€ - ì½”
        (2, 4),   # ìš°ê·€ - ì½”  
        (0, 1),   # ì½”ë - ì¢Œê·€
        (0, 2),   # ì½”ë - ìš°ê·€
        
        # ëª¸í†µ
        (5, 1),   # ëª© - ì¢Œê·€
        (5, 2),   # ëª© - ìš°ê·€
        (5, 6),   # ëª© - ì¢Œì–´ê¹¨
        (5, 7),   # ëª© - ìš°ì–´ê¹¨
        (6, 12),  # ì¢Œì–´ê¹¨ - ì¢Œì—‰ë©ì´
        (7, 13),  # ìš°ì–´ê¹¨ - ìš°ì—‰ë©ì´
        (12, 13), # ì¢Œì—‰ë©ì´ - ìš°ì—‰ë©ì´
        
        # ì•ë‹¤ë¦¬
        (6, 8),   # ì¢Œì–´ê¹¨ - ì¢ŒíŒ”ê¿ˆì¹˜
        (8, 10),  # ì¢ŒíŒ”ê¿ˆì¹˜ - ì¢Œë°œëª©
        (7, 9),   # ìš°ì–´ê¹¨ - ìš°íŒ”ê¿ˆì¹˜
        (9, 11),  # ìš°íŒ”ê¿ˆì¹˜ - ìš°ë°œëª©
        
        # ë’·ë‹¤ë¦¬
        (12, 14), # ì¢Œì—‰ë©ì´ - ì¢Œë¬´ë¦
        (14, 16), # ì¢Œë¬´ë¦ - ê¼¬ë¦¬ë (ì„ì‹œ)
        (13, 15), # ìš°ì—‰ë©ì´ - ìš°ë¬´ë¦
        (15, 16), # ìš°ë¬´ë¦ - ê¼¬ë¦¬ë (ì„ì‹œ)
    ]
    
    # BGR ìƒ‰ìƒ ì •ì˜ (OpenCVëŠ” BGR ìˆœì„œ)
    colors_bgr = {
        'keypoint': (0, 255, 255),      # ë…¸ë€ìƒ‰ í‚¤í¬ì¸íŠ¸
        'head': (0, 0, 255),            # ğŸ”´ ë¹¨ê°„ìƒ‰ - ë¨¸ë¦¬ ë¶€ë¶„
        'body': (0, 255, 0),            # ğŸŸ¢ ì´ˆë¡ìƒ‰ - ëª¸í†µ
        'front_legs': (0, 255, 255),    # ğŸŸ¡ ë…¸ë€ìƒ‰ - ì•ë‹¤ë¦¬
        'back_legs': (0, 165, 255),     # ğŸŸ  ì£¼í™©ìƒ‰ - ë’·ë‹¤ë¦¬
    }
    
    # RGBë¥¼ BGRë¡œ ë³€í™˜ (OpenCV ì²˜ë¦¬ìš©)
    img_bgr = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)
    
    # íˆ¬ëª…ë„ ì ìš©ì„ ìœ„í•œ ì˜¤ë²„ë ˆì´ ìƒì„±
    overlay = img_bgr.copy()
    alpha = 0.5  # 50% íˆ¬ëª…ë„
    
    for pose_result in pose_results:
        if hasattr(pose_result, 'pred_instances'):
            keypoints = pose_result.pred_instances.keypoints
            scores = pose_result.pred_instances.keypoint_scores
            
            # í…ì„œì¸ ê²½ìš° numpyë¡œ ë³€í™˜
            if hasattr(keypoints, 'cpu'):
                keypoints = keypoints.cpu().numpy()
            if hasattr(scores, 'cpu'):
                scores = scores.cpu().numpy()
        else:
            keypoints = pose_result['keypoints']
            scores = pose_result.get('keypoint_scores', keypoints[..., 2:3] if keypoints.shape[-1] > 2 else None)
        
        # ì²« ë²ˆì§¸ ì¸ìŠ¤í„´ìŠ¤ì˜ í‚¤í¬ì¸íŠ¸ë§Œ ì‹œê°í™”
        if len(keypoints) > 0:
            kpts = keypoints[0]  # (17, 2) ë˜ëŠ” (17, 3)
            kpt_scores = scores[0] if scores is not None else np.ones(len(kpts))
            
            confidence_threshold = 0.3
            
            # ê³¨ê²© ì„  ê·¸ë¦¬ê¸° (ì˜¤ë²„ë ˆì´ì—)
            for connection in skeleton_connections:
                pt1_idx, pt2_idx = connection
                if (pt1_idx < len(kpts) and pt2_idx < len(kpts) and 
                    kpt_scores[pt1_idx] > confidence_threshold and 
                    kpt_scores[pt2_idx] > confidence_threshold):
                    
                    pt1 = (int(kpts[pt1_idx][0]), int(kpts[pt1_idx][1]))
                    pt2 = (int(kpts[pt2_idx][0]), int(kpts[pt2_idx][1]))
                    
                    # ì—°ê²°ì„  ìƒ‰ìƒ ê²°ì •
                    if connection in [(1, 2), (1, 3), (2, 4), (0, 1), (0, 2), (5, 1), (5, 2)]:
                        line_color = colors_bgr['head']
                    elif connection in [(5, 6), (5, 7), (6, 12), (7, 13), (12, 13)]:
                        line_color = colors_bgr['body']
                    elif connection in [(6, 8), (8, 10), (7, 9), (9, 11)]:
                        line_color = colors_bgr['front_legs']
                    else:
                        line_color = colors_bgr['back_legs']
                    
                    cv2.line(overlay, pt1, pt2, line_color, 1, lineType=cv2.LINE_AA)
            
            # í‚¤í¬ì¸íŠ¸ ì  ê·¸ë¦¬ê¸° (ì˜¤ë²„ë ˆì´ì—)
            for i, (kpt, score) in enumerate(zip(kpts, kpt_scores)):
                if score > confidence_threshold:
                    pt = (int(kpt[0]), int(kpt[1]))
                    
                    # í‚¤í¬ì¸íŠ¸ë³„ ìƒ‰ìƒ (BGR í˜•ì‹)
                    if i in [0, 1, 2, 3, 4]:  # ë¨¸ë¦¬ ë¶€ë¶„
                        kpt_color = (0, 0, 255)      # ë¹¨ê°„ìƒ‰
                    elif i == 5:  # ëª©
                        kpt_color = (0, 255, 0)      # ì´ˆë¡ìƒ‰
                    elif i in [6, 7, 8, 9, 10, 11]:  # ì•ë‹¤ë¦¬
                        kpt_color = (0, 255, 255)    # ë…¸ë€ìƒ‰
                    else:  # ë’·ë‹¤ë¦¬, ê¼¬ë¦¬
                        kpt_color = (0, 165, 255)    # ì£¼í™©ìƒ‰
                    
                    # í‚¤í¬ì¸íŠ¸ ì› ê·¸ë¦¬ê¸° (í¬ê¸° ì¶•ì†Œ)
                    cv2.circle(overlay, pt, 3, kpt_color, -1)
                    cv2.circle(overlay, pt, 3, (255, 255, 255), 1)  # í°ìƒ‰ í…Œë‘ë¦¬
    
    # íˆ¬ëª…ë„ ì ìš©í•˜ì—¬ í•©ì„±
    result = cv2.addWeighted(img_bgr, 1-alpha, overlay, alpha, 0)
    
    # BGRì„ ë‹¤ì‹œ RGBë¡œ ë³€í™˜
    return cv2.cvtColor(result, cv2.COLOR_BGR2RGB)

if __name__ == "__main__":
    # --- AP-10K ëª¨ë¸ ë° ì‹œê°í™” ë„êµ¬ ë¡œë“œ ---
    # setup_ap10k_modelì˜ ë°˜í™˜ ê°’ì— dataset_infoê°€ ì—†ìŠµë‹ˆë‹¤.
    ap10k_model, ap10k_device, visualizer = setup_ap10k_model()

    # --- SimCLR ìœ ì‚¬ ê°•ì•„ì§€ ê²€ìƒ‰ ---
    query_image_path = QUERY_IMAGE_PATH 

    if not os.path.exists(query_image_path):
        print(f"ì˜¤ë¥˜: ì¿¼ë¦¬ ì´ë¯¸ì§€ '{query_image_path}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ ìˆ˜ì •í•˜ê±°ë‚˜ íŒŒì¼ì„ ì¤€ë¹„í•´ì£¼ì„¸ìš”.")
    else:
        print(f"\nì¿¼ë¦¬ ì´ë¯¸ì§€: {query_image_path}")
        print("SimCLR ê¸°ë°˜ ìœ ì‚¬ ê°•ì•„ì§€ ê²€ìƒ‰ ì‹œì‘...")
        
        # ë™ì ìœ¼ë¡œ search_similar_dogs í•¨ìˆ˜ ì„í¬íŠ¸ (ìˆœí™˜ ì°¸ì¡° ë°©ì§€)
        try:
            from training.search_similar_dogs import search_similar_dogs
        except ImportError:
            try:
                from .search_similar_dogs import search_similar_dogs
            except ImportError:
                from search_similar_dogs import search_similar_dogs
        
        top_similar_dogs_simclr = search_similar_dogs(
            query_image_path=query_image_path, 
            top_k=5,
            model_path=SIMCLR_MODEL_PATH,
            out_dim=SIMCLR_OUT_DIM,
            image_size=SIMCLR_IMAGE_SIZE,
            db_features_file=DB_FEATURES_FILE,
            db_image_paths_file=DB_IMAGE_PATHS_FILE
        )
        
        print("\n--- SimCLR ê¸°ë°˜ ê°€ì¥ ìœ ì‚¬í•œ ê°•ì•„ì§€ ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ 5ê°œ) ---")
        # ì¿¼ë¦¬ ì´ë¯¸ì§€ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ë° ì‹œê°í™”

        print(f"\nì¿¼ë¦¬ ì´ë¯¸ì§€ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ë° ì‹œê°í™”: {query_image_path}")
        query_kp_output_path, query_pose_results = detect_and_visualize_keypoints(
            query_image_path, ap10k_model, ap10k_device, visualizer
        )
        print("[DEBUG] ì¿¼ë¦¬ pose_results type:", type(query_pose_results))
        print("[DEBUG] ì¿¼ë¦¬ pose_results len:", len(query_pose_results) if query_pose_results else 0)
        if query_pose_results:
            print("[DEBUG] ì¿¼ë¦¬ pose_results[0] type:", type(query_pose_results[0]))
            print("[DEBUG] ì¿¼ë¦¬ pose_results[0] keys:", list(query_pose_results[0].keys()) if isinstance(query_pose_results[0], dict) else "(not dict)")
            if isinstance(query_pose_results[0], dict) and 'keypoints' in query_pose_results[0]:
                print("[DEBUG] ì¿¼ë¦¬ keypoints shape:", query_pose_results[0]['keypoints'].shape)
                print("[DEBUG] ì¿¼ë¦¬ keypoints sample:\n", query_pose_results[0]['keypoints'])
            else:
                print("[DEBUG] ì¿¼ë¦¬ pose_results[0]ì— 'keypoints' ì—†ìŒ ë˜ëŠ” íƒ€ì… ë¶ˆì¼ì¹˜")
        else:
            print("[DEBUG] ì¿¼ë¦¬ pose_resultsê°€ None ë˜ëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸ì„")

        final_results = []

        # SimCLR ê²€ìƒ‰ ê²°ê³¼ê°€ dict ë˜ëŠ” íŠœí”Œì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìë™ íŒë³„
        for i, result in enumerate(top_similar_dogs_simclr):
            if isinstance(result, dict):
                simclr_score = result.get('similarity', 0.0)
                db_img_path = result.get('image_url') or result.get('image_path')
            else:
                simclr_score, db_img_path = result

            print(f"\n--- ìœ ì‚¬ ê°•ì•„ì§€ {i+1}: {db_img_path} (SimCLR ìœ ì‚¬ë„: {simclr_score:.4f}) ---")
            db_kp_output_path, db_pose_results = detect_and_visualize_keypoints(
                db_img_path, ap10k_model, ap10k_device, visualizer
            )

            print(f"[DEBUG] DB pose_results for {db_img_path} type:", type(db_pose_results))
            print(f"[DEBUG] DB pose_results for {db_img_path} len:", len(db_pose_results) if db_pose_results else 0)
            if db_pose_results:
                print(f"[DEBUG] DB pose_results[0] type:", type(db_pose_results[0]))
                print(f"[DEBUG] DB pose_results[0] keys:", list(db_pose_results[0].keys()) if isinstance(db_pose_results[0], dict) else "(not dict)")
                if isinstance(db_pose_results[0], dict) and 'keypoints' in db_pose_results[0]:
                    print(f"[DEBUG] DB keypoints shape:", db_pose_results[0]['keypoints'].shape)
                    print(f"[DEBUG] DB keypoints sample:\n", db_pose_results[0]['keypoints'])
                else:
                    print(f"[DEBUG] DB pose_results[0]ì— 'keypoints' ì—†ìŒ ë˜ëŠ” íƒ€ì… ë¶ˆì¼ì¹˜")
            else:
                print(f"[DEBUG] DB pose_resultsê°€ None ë˜ëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸ì„")

            keypoint_similarity = 0.0
            if query_pose_results and db_pose_results:
                keypoint_similarity = calculate_keypoint_similarity(query_pose_results, db_pose_results, SIMCLR_IMAGE_SIZE)
                print(f"í‚¤í¬ì¸íŠ¸ ìœ ì‚¬ë„: {keypoint_similarity:.4f}")
            else:
                print("í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ê²°ê³¼ ì—†ìŒ.")
            
            final_score = (simclr_score * 0.7) + (keypoint_similarity * 0.3) 
            final_results.append((final_score, db_img_path, simclr_score, keypoint_similarity))
            
            print(f"ìµœì¢… ë³µí•© ìœ ì‚¬ë„: {final_score:.4f}")

        final_results.sort(key=lambda x: x[0], reverse=True)

        print("\n--- ìµœì¢… ë³µí•© ìœ ì‚¬ë„ ê¸°ì¤€ ê²€ìƒ‰ ê²°ê³¼ ---")
        for i, (final_score, path, simclr_s, kp_s) in enumerate(final_results):
            print(f"{i+1}. ìµœì¢… ìœ ì‚¬ë„: {final_score:.4f}, SimCLR: {simclr_s:.4f}, KP: {kp_s:.4f}, ì´ë¯¸ì§€ ê²½ë¡œ: {path}")