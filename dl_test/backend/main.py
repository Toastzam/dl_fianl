from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.responses import JSONResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
import os
import sys
import json
import tempfile
import io
import base64
import numpy as np
from PIL import Image
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import ViTModel, ViTConfig, ViTImageProcessor
import cv2
from matplotlib import cm

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# ìš°ë¦¬ê°€ ë§Œë“  ëª¨ë“ˆ ì„í¬íŠ¸
from training.visualize_keypoints import (
    setup_ap10k_model, 
    detect_and_visualize_keypoints,
    calculate_keypoint_similarity
)
from training.search_similar_dogs import search_similar_dogs

class SimCLREncoder(nn.Module):
    def __init__(self, model_name="google/vit-base-patch16-224-in21k", projection_dim=128):
        super().__init__()
        config = ViTConfig.from_pretrained(model_name)
        # ViTModelì„ ì‚¬ìš©í•˜ë©´, ë‚´ë¶€ì ìœ¼ë¡œ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ì¶”ì¶œí•˜ê¸° ìœ„í•œ hookì„ ê±¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # ì‹¤ì œ ì–´í…ì…˜ ë§µ ì‹œê°í™”ë¥¼ ìœ„í•´ì„œëŠ” ë” ë³µì¡í•œ ë¡œì§ì´ í•„ìš”í•©ë‹ˆë‹¤.
        self.vit = ViTModel.from_pretrained(model_name, output_attentions=True) # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì¶œë ¥ í™œì„±í™”
        self.projection_head = nn.Sequential(
            nn.Linear(config.hidden_size, config.hidden_size),
            nn.ReLU(),
            nn.Linear(config.hidden_size, projection_dim)
        )
        self.model_name = model_name

    def forward(self, pixel_values):
        # ViTì˜ ë§ˆì§€ë§‰ hidden stateì˜ CLS í† í°ë§Œ ì‚¬ìš©
        # output_attentions=True ì„¤ì •í•˜ë©´ outputs.attentions ì— ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë„ ë°˜í™˜ë¨
        outputs = self.vit(pixel_values=pixel_values)
        vit_output = outputs.last_hidden_state[:, 0, :] # CLS token output
        projection = self.projection_head(vit_output)
        return projection, outputs.attentions # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë„ í•¨ê»˜ ë°˜í™˜

# --- FastAPI ì•± ì´ˆê¸°í™” ---

# --- ì „ì—­ ë³€ìˆ˜ ì„¤ì • ---
model = None
processor = None
ap10k_model = None
ap10k_device = None
visualizer = None
device = "cuda" if torch.cuda.is_available() else "cpu"

# ì—…ë¡œë“œ í´ë” ì„¤ì •
UPLOAD_FOLDER = "uploads"
ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'gif'}
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs("output_keypoints", exist_ok=True)

def allowed_file(filename: str) -> bool:
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

# --- ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ (ì•±ì´ ì²˜ìŒ ì‹¤í–‰ë  ë•Œ í•œ ë²ˆë§Œ) ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    # ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ
    global ap10k_model, device, visualizer
    try:
        print("ğŸš€ AP-10K ëª¨ë¸ ë¡œë”© ì¤‘...")
        ap10k_model, device, visualizer = setup_ap10k_model()
        print("âœ… AP-10K ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        ap10k_model = None
    
    yield
    
    # ì¢…ë£Œ ì‹œ ì •ë¦¬
    print("ğŸ”„ ì„œë²„ ì¢…ë£Œ ì¤‘...")

# FastAPI ì•± ìƒì„± ì‹œ lifespan ì‚¬ìš©
app = FastAPI(
    title="Dog Similarity Search API",
    description="SimCLR + AP-10K í‚¤í¬ì¸íŠ¸ ê¸°ë°˜ ê°•ì•„ì§€ ìœ ì‚¬ë„ ê²€ìƒ‰ API",
    version="1.0.0",
    lifespan=lifespan
)

# CORS ì„¤ì • ì¶”ê°€
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],  # React ì•± URL
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì •ì  íŒŒì¼ (ì´ë¯¸ì§€) ì„œë¹™ ì„¤ì •
app.mount("/static", StaticFiles(directory="static"), name="static")
app.mount("/uploads", StaticFiles(directory="uploads"), name="uploads")
app.mount("/output_keypoints", StaticFiles(directory="output_keypoints"), name="output_keypoints")

# --- íˆíŠ¸ë§µ ìƒì„± í•¨ìˆ˜ (ë§¤ìš° ê°„ëµí™”ëœ ì˜ˆì‹œ) ---
# ì‹¤ì œ ì–´í…ì…˜ ë§µ ì‹œê°í™”ë¥¼ ìœ„í•´ì„œëŠ” ViTModelì˜ outputs.attentionsë¥¼ ì‚¬ìš©í•˜ì—¬
# ê° í—¤ë“œì˜ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ì¡°í•©í•˜ê³ , ì´ë¥¼ ì´ë¯¸ì§€ í¬ê¸°ì— ë§ê²Œ ì—…ìƒ˜í”Œë§í•´ì•¼ í•©ë‹ˆë‹¤.
# ì´ëŠ” ë³µì¡í•œ ì‘ì—…ì´ë¯€ë¡œ, í˜„ì¬ëŠ” ì‹œê°í™” íš¨ê³¼ë¥¼ ìœ„í•œ "ë”ë¯¸" íˆíŠ¸ë§µì„ ìƒì„±í•©ë‹ˆë‹¤.
def generate_dummy_heatmap_b64(image_pil: Image.Image, value=0.5):
    # ì…ë ¥ ì´ë¯¸ì§€ì™€ ë™ì¼í•œ í¬ê¸°ì˜ íˆíŠ¸ë§µ ë°ì´í„°ë¥¼ ìƒì„± (0.0 ~ 1.0)
    heatmap_data = np.full((image_pil.height, image_pil.width), value, dtype=np.float32)

    # ì¤‘ì•™ì— ë°ì€ ì›ì„ ê·¸ë¦¬ëŠ” ì˜ˆì‹œ (ì‹¤ì œ ì–´í…ì…˜ ë§µì´ ì•„ë‹˜)
    center_x, center_y = image_pil.width // 2, image_pil.height // 2
    radius = min(image_pil.width, image_pil.height) // 4
    cv2.circle(heatmap_data, (center_x, center_y), radius, 1.0, -1) # ì›í˜•ìœ¼ë¡œ ê°•ì¡°

    # íˆíŠ¸ë§µ ë°ì´í„°ë¥¼ ì»¬ëŸ¬ë§µìœ¼ë¡œ ë³€í™˜ (matplotlibì˜ jet ì»¬ëŸ¬ë§µ ì‚¬ìš©)
    # RGBA (Red, Green, Blue, Alpha) í˜•íƒœë¡œ ë‚˜ì˜´
    heatmap_colored = cm.jet(heatmap_data)
    # 0-1 ìŠ¤ì¼€ì¼ì˜ RGBAë¥¼ 0-255 ìŠ¤ì¼€ì¼ì˜ RGBAë¡œ ë³€í™˜
    heatmap_colored_uint8 = (heatmap_colored * 255).astype(np.uint8)

    # PIL Image ê°ì²´ë¡œ ë³€í™˜
    heatmap_pil = Image.fromarray(heatmap_colored_uint8, 'RGBA')

    # PIL Imageë¥¼ Bytesë¡œ ë³€í™˜ (PNG í˜•ì‹)
    buffer = io.BytesIO()
    heatmap_pil.save(buffer, format="PNG")
    heatmap_bytes = buffer.getvalue()
    return base64.b64encode(heatmap_bytes).decode('utf-8')

# ê¸°ì¡´ generate_dummy_heatmap_b64ëŠ” ì„ì‹œë¡œ ë‚¨ê²¨ë‘ê³ ,
# ì‹¤ì œ ì–´í…ì…˜ ë§µì„ ìœ„í•œ í•¨ìˆ˜ë¥¼ ìƒˆë¡œ ë§Œë“­ë‹ˆë‹¤.
# ì£¼ì˜: ì´ ì½”ë“œëŠ” ê°œë… ì„¤ëª…ì´ë©°, ì‹¤ì œ ë™ì‘í•˜ëŠ” ì™„ì „í•œ ì½”ë“œëŠ” ì•„ë‹™ë‹ˆë‹¤.
# ViTì˜ ë‚´ë¶€ êµ¬ì¡°ì™€ attention map ì¶”ì¶œ ë°©ë²•ì— ëŒ€í•œ ê¹Šì€ ì´í•´ê°€ í•„ìš”í•©ë‹ˆë‹¤.
def get_attention_heatmap(original_image_pil: Image.Image, attentions, patch_size=16):
    # attentionsëŠ” outputs.attentions ì…ë‹ˆë‹¤.
    # ì¼ë°˜ì ìœ¼ë¡œ ViTì˜ attentionsëŠ” (num_layers, batch_size, num_heads, seq_len, seq_len) í˜•íƒœì…ë‹ˆë‹¤.
    # ìš°ë¦¬ëŠ” ë§ˆì§€ë§‰ ë ˆì´ì–´ì˜ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ê³ ,
    # íŠ¹íˆ [CLS] í† í°ì´ ë‹¤ë¥¸ íŒ¨ì¹˜ë“¤ì— ì–¼ë§ˆë‚˜ ì§‘ì¤‘í–ˆëŠ”ì§€ë¥¼ ì‹œê°í™”í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤.

    # 1. CLS í† í°ì— ëŒ€í•œ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì¶”ì¶œ (ì˜ˆì‹œ)
    # outputs.attentionsëŠ” íŠœí”Œ í˜•íƒœì´ë¯€ë¡œ, ë§ˆì§€ë§‰ ë ˆì´ì–´ ([0] ì¸ë±ìŠ¤)ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    # attentions_last_layer = attentions[-1] # (batch_size, num_heads, seq_len, seq_len)

    # ëª¨ë“  í—¤ë“œì˜ CLS í† í° (ì¸ë±ìŠ¤ 0)ì´ ë‹¤ë¥¸ íŒ¨ì¹˜ë“¤ (ì¸ë±ìŠ¤ 1ë¶€í„°)ì— ëŒ€í•œ í‰ê·  ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°
    # ì˜ˆ: attention_weights = attentions_last_layer[0, :, 0, 1:].mean(dim=0)
    # (ì´ ë¶€ë¶„ì€ ëª¨ë¸ì˜ ì •í™•í•œ êµ¬ì¡°ì™€ ì›í•˜ëŠ” ì‹œê°í™” ë°©ì‹ì— ë”°ë¼ ë‹¬ë¼ì§‘ë‹ˆë‹¤.)
    
    # -------------------------------------------------------------
    # ì—¬ê¸°ì„œëŠ” ì„ì‹œë¡œ ViT ëª¨ë¸ ë‚´ë¶€ì˜ Attention ê°€ì¤‘ì¹˜ë¥¼ ì§ì ‘ ì ‘ê·¼í•˜ëŠ” ì˜ˆì‹œë¥¼ ë³´ì—¬ë“œë¦½ë‹ˆë‹¤.
    # ì‹¤ì œë¡œëŠ” model.vit.encoder.layer[-1].attention.self.get_attn_map() ê°™ì€
    # íŠ¹ì • hookì´ë‚˜ ë©”ì„œë“œë¥¼ í†µí•´ ê°€ì ¸ì˜¤ëŠ” ê²ƒì´ ë” ì•ˆì •ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # DINO ëª¨ë¸ ê°™ì€ ê²½ìš°ëŠ” ê³µì‹ ê¹ƒí—ˆë¸Œì— ì‹œê°í™” ì½”ë“œê°€ ì˜ ì œê³µë©ë‹ˆë‹¤.
    # -------------------------------------------------------------

    # **ê°€ì¥ ê°„ë‹¨í•˜ê²Œ ViTì—ì„œ ì–´í…ì…˜ ë§µì„ ì–»ëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜ (ê°œë…ì  ì½”ë“œ):**
    # ViTModel.forward ì‹œ output_attentions=True ì„¤ì •í•˜ë©´ outputs.attentionsê°€ ë°˜í™˜ë©ë‹ˆë‹¤.
    # outputs.attentionsëŠ” ê° ë ˆì´ì–´ì˜ ì–´í…ì…˜ í…ì„œë“¤ì˜ íŠœí”Œì…ë‹ˆë‹¤.
    # ê°€ì¥ ë§ˆì§€ë§‰ ë ˆì´ì–´ì˜ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ (outputs.attentions[-1])ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    # ì´ ê°€ì¤‘ì¹˜ëŠ” (batch_size, num_heads, sequence_length, sequence_length) í˜•íƒœì…ë‹ˆë‹¤.
    # ì—¬ê¸°ì„œ sequence_lengthëŠ” 1 (CLS í† í°) + (ì´ë¯¸ì§€ íŒ¨ì¹˜ ìˆ˜) ì…ë‹ˆë‹¤.
    
    # CLS í† í° (ì¸ë±ìŠ¤ 0)ì´ ë‹¤ë¥¸ íŒ¨ì¹˜ë“¤ì— ëŒ€í•œ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    # ëª¨ë“  í—¤ë“œì˜ í‰ê· ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤.
    # ì˜ˆì‹œ: attention_map = attentions[-1][0, :, 0, 1:].mean(dim=0) # [num_patches]
    
    # ì§€ê¸ˆì€ ì‹¤ì œ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì¶”ì¶œ ë¡œì§ ëŒ€ì‹ , 'ë”ë¯¸'ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.
    # ì´ ë¶€ë¶„ì„ ì§ì ‘ êµ¬í˜„í•˜ì…”ì•¼ í•©ë‹ˆë‹¤!
    
    # **ì„ì‹œë¡œ, ë‹¤ì‹œ ë”ë¯¸ íˆíŠ¸ë§µì„ ìƒì„±í•˜ì—¬ ì „ë‹¬í•©ë‹ˆë‹¤.**
    # (ì‚¬ìš©ìë‹˜ì´ ì´ í•¨ìˆ˜ë¥¼ ì‹¤ì œ ì–´í…ì…˜ ë§µ ìƒì„± ë¡œì§ë¡œ êµì²´í•´ì•¼ í•¨)
    # -----------------------------------------------------------------
    
    # ì‹¤ì œ ì–´í…ì…˜ ë§µ ë°ì´í„° (0~1 ì‚¬ì´)
    # ì´ ë¶€ë¶„ì— ìœ„ì—ì„œ ê³„ì‚°í•œ `attention_map`ì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
    # attention_map = attention_map.cpu().numpy().reshape(image_width // patch_size, image_height // patch_size)
    # ì´í›„ resizeí•˜ì—¬ ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ë§ì¶¤

    # í˜„ì¬ëŠ” ì•„ë˜ generate_dummy_heatmap_b64ë¥¼ ê³„ì† ì‚¬ìš©í•©ë‹ˆë‹¤.
    # ë”°ë¼ì„œ ê³„ì† ì¤‘ì•™ì— ì›í˜• íˆíŠ¸ë§µì´ ë‚˜ì˜¬ ê²ƒì…ë‹ˆë‹¤.
    return generate_dummy_heatmap_b64(original_image_pil) # <<< ì´ ë¶€ë¶„ì„ ì‹¤ì œ ë¡œì§ìœ¼ë¡œ êµì²´í•´ì•¼ í•¨


def get_attention_heatmap(original_image_pil: Image.Image, attentions, patch_size=16):
    heatmap_data = np.full((original_image_pil.height, original_image_pil.width), 0.5, dtype=np.float32)
    center_x, center_y = original_image_pil.width // 2, original_image_pil.height // 2
    radius = min(original_image_pil.width, original_image_pil.height) // 4
    cv2.circle(heatmap_data, (center_x, center_y), radius, 1.0, -1)
    heatmap_colored = cm.jet(heatmap_data)
    heatmap_colored_uint8 = (heatmap_colored * 255).astype(np.uint8)
    heatmap_pil = Image.fromarray(heatmap_colored_uint8, 'RGBA')
    buffer = io.BytesIO()
    heatmap_pil.save(buffer, format="PNG")
    heatmap_bytes = buffer.getvalue()
    heatmap_b64 = base64.b64encode(heatmap_bytes).decode('utf-8')
    return heatmap_b64, heatmap_data


# --- API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜ ---
@app.post("/compare_dogs_with_heatmap/")
async def compare_dog_images_with_heatmap(file1: UploadFile = File(...), file2: UploadFile = File(...)):
    if model is None or processor is None:
        return JSONResponse(status_code=503, content={"message": "ëª¨ë¸ì´ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."})

    try:
        # 1. ì´ë¯¸ì§€ ì½ê¸° (PIL Image ê°ì²´ë¡œ)
        image1_pil = Image.open(io.BytesIO(await file1.read())).convert("RGB")
        image2_pil = Image.open(io.BytesIO(await file2.read())).convert("RGB")

        # 2. ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë° ëª¨ë¸ ì…ë ¥ ì¤€ë¹„
        # ViTImageProcessorë¥¼ ì‚¬ìš©í•˜ë©´ í¸ë¦¬í•˜ê²Œ ì „ì²˜ë¦¬ ê°€ëŠ¥
        inputs1 = processor(images=image1_pil, return_tensors="pt").to(device)
        inputs2 = processor(images=image2_pil, return_tensors="pt").to(device)

        # 3. ëª¨ë¸ ìˆœì „íŒŒ (íŠ¹ì§• ì¶”ì¶œ ë° ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì–»ê¸°)
        with torch.no_grad():
            features1, attentions1 = model(inputs1['pixel_values'])
            features2, attentions2 = model(inputs2['pixel_values'])

        # 4. ìœ ì‚¬ë„ ê³„ì‚°
        # SimCLRì˜ projection_head ì¶œë ¥ì„ ì‚¬ìš©í•´ì•¼ í•˜ì§€ë§Œ,
        # ìœ ì‚¬ë„ ì‹œê°í™”ë¥¼ ìœ„í•´ feature(CLS token output)ë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
        # ì—¬ê¸°ì„œëŠ” features (CLS token output) ì‚¬ìš©
        similarity = F.cosine_similarity(features1, features2).item()

        # 5. íˆíŠ¸ë§µ ìƒì„± ë° Base64 ì¸ì½”ë”©
        # ì‹¤ì œ ì–´í…ì…˜ ë§µì„ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜ë¡œ êµì²´ í•„ìš”!
        heatmap_b64_1, heatmap1 = get_attention_heatmap(image1_pil, attentions1)
        heatmap_b64_2, heatmap2 = get_attention_heatmap(image2_pil, attentions2)
        
        # ì˜ˆì‹œ: ë”ë¯¸ ì¢Œí‘œ (ì¤‘ì•™)
        point1 = {"x": image1_pil.width // 2, "y": image1_pil.height // 2}
        point2 = {"x": image2_pil.width // 2, "y": image2_pil.height // 2}

        y1, x1 = np.unravel_index(np.argmax(heatmap1), heatmap1.shape)
        y2, x2 = np.unravel_index(np.argmax(heatmap2), heatmap2.shape)
        point1 = {"x": int(x1), "y": int(y1)}
        point2 = {"x": int(x2), "y": int(y2)}

        return JSONResponse({
            "similarity": similarity,
            "heatmap_image1": f"data:image/png;base64,{heatmap_b64_1}",
            "heatmap_image2": f"data:image/png;base64,{heatmap_b64_2}",
            "point1": point1,  # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ì˜ ìœ ì‚¬ ë¶€ìœ„ ì¢Œí‘œ
            "point2": point2,  # ë‘ ë²ˆì§¸ ì´ë¯¸ì§€ì˜ ìœ ì‚¬ ë¶€ìœ„ ì¢Œí‘œ
            "message": "ê°•ì•„ì§€ ìœ ì‚¬ë„ ë¹„êµ ë° íˆíŠ¸ë§µ ìƒì„± ì™„ë£Œ!"
        })

    except Exception as e:
        print(f"API ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return JSONResponse(status_code=500, content={"message": f"ì„œë²„ ì˜¤ë¥˜: {str(e)}"})

# --- ìƒˆë¡œìš´ API: ì´ë¯¸ì§€ ì—…ë¡œë“œ ë° ìœ ì‚¬ë„ ê²€ìƒ‰ ---
@app.post("/api/upload_and_search/")
async def upload_and_search_similar_dogs(file: UploadFile = File(...)):
    """ì´ë¯¸ì§€ ì—…ë¡œë“œ ë° ìœ ì‚¬ë„ ê²€ìƒ‰ API"""
    if model is None or processor is None:
        raise HTTPException(status_code=503, detail="SimCLR ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    if ap10k_model is None:
        raise HTTPException(status_code=503, detail="AP-10K í‚¤í¬ì¸íŠ¸ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    try:
        # 1. ì—…ë¡œë“œëœ ì´ë¯¸ì§€ ì €ì¥
        file_content = await file.read()
        filename = f"query_{file.filename}"
        filepath = os.path.join(UPLOAD_FOLDER, filename)
        
        with open(filepath, "wb") as f:
            f.write(file_content)
        
        print(f"ğŸ” ì¿¼ë¦¬ ì´ë¯¸ì§€ ì €ì¥: {filepath}")
        
        # 2. ì¿¼ë¦¬ ì´ë¯¸ì§€ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ
        query_kp_output_path, query_pose_results = detect_and_visualize_keypoints(
            filepath, ap10k_model, ap10k_device, visualizer
        )
        
        if query_pose_results is None:
            raise HTTPException(status_code=500, detail="í‚¤í¬ì¸íŠ¸ ê²€ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        # 3. SimCLR ê¸°ë°˜ ìœ ì‚¬ ì´ë¯¸ì§€ ê²€ìƒ‰
        print("ğŸ” SimCLR ê¸°ë°˜ ìœ ì‚¬ ì´ë¯¸ì§€ ê²€ìƒ‰...")
        similar_results = search_similar_dogs(
            query_image_path=filepath,
            top_k=5,
            model_path="../models/simclr_vit_dog_model.pth",
            out_dim=128,
            image_size=224,
            db_features_file="../db_features.npy",
            db_image_paths_file="../db_image_paths.npy"
        )
        
        # 4. ê° ìœ ì‚¬ ì´ë¯¸ì§€ì— ëŒ€í•´ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ë° ìœ ì‚¬ë„ ê³„ì‚°
        results = []
        for i, (simclr_similarity, similar_path) in enumerate(similar_results):
            print(f"ğŸ” ìœ ì‚¬ ì´ë¯¸ì§€ {i+1} í‚¤í¬ì¸íŠ¸ ê²€ì¶œ: {similar_path}")
            
            # í‚¤í¬ì¸íŠ¸ ê²€ì¶œ
            similar_kp_output_path, similar_pose_results = detect_and_visualize_keypoints(
                similar_path, ap10k_model, ap10k_device, visualizer
            )
            
            # í‚¤í¬ì¸íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°
            keypoint_similarity = 0.0
            if similar_pose_results is not None:
                keypoint_similarity = calculate_keypoint_similarity(
                    query_pose_results, similar_pose_results
                )
            
            # ë³µí•© ìœ ì‚¬ë„ ê³„ì‚° (SimCLR 70% + í‚¤í¬ì¸íŠ¸ 30%)
            combined_similarity = (0.7 * simclr_similarity) + (0.3 * keypoint_similarity)
            
            results.append({
                'rank': i + 1,
                'image_path': similar_path.replace('\\', '/'),
                'keypoint_image_path': similar_kp_output_path.replace('\\', '/') if similar_kp_output_path else None,
                'simclr_similarity': float(simclr_similarity),
                'keypoint_similarity': float(keypoint_similarity),
                'combined_similarity': float(combined_similarity)
            })
            
            print(f"  âœ… SimCLR: {simclr_similarity:.4f}, í‚¤í¬ì¸íŠ¸: {keypoint_similarity:.4f}, ë³µí•©: {combined_similarity:.4f}")
        
        # ë³µí•© ìœ ì‚¬ë„ë¡œ ì¬ì •ë ¬
        results.sort(key=lambda x: x['combined_similarity'], reverse=True)
        
        return JSONResponse({
            'success': True,
            'query_image': filepath.replace('\\', '/'),
            'query_keypoint_image': query_kp_output_path.replace('\\', '/') if query_kp_output_path else None,
            'results': results
        })
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise HTTPException(status_code=500, detail=f"ì„œë²„ ì˜¤ë¥˜: {str(e)}")

@app.get("/api/image/{file_path:path}")
async def serve_image(file_path: str):
    """ì´ë¯¸ì§€ íŒŒì¼ ì„œë¹™"""
    try:
        # ê²½ë¡œ ì •ê·œí™”
        file_path = file_path.replace('/', os.sep)
        
        # ë‹¤ì–‘í•œ ê²½ë¡œì—ì„œ ì´ë¯¸ì§€ ì°¾ê¸°
        possible_paths = [
            file_path,
            os.path.join('uploads', file_path),
            os.path.join('output_keypoints', file_path),
            os.path.join('training', file_path),
            os.path.join('..', file_path)  # ìƒìœ„ ë””ë ‰í† ë¦¬ë„ ê²€ìƒ‰
        ]
        
        for path in possible_paths:
            full_path = os.path.abspath(path)
            if os.path.exists(full_path) and os.path.isfile(full_path):
                print(f"ğŸ“· ì´ë¯¸ì§€ ì„œë¹™: {full_path}")
                return FileResponse(full_path)
        
        print(f"âŒ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {file_path}")
        print(f"ì‹œë„í•œ ê²½ë¡œë“¤: {possible_paths}")
        raise HTTPException(status_code=404, detail="ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
    except Exception as e:
        print(f"âŒ ì´ë¯¸ì§€ ì„œë¹™ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# --- í—¬ìŠ¤ ì²´í¬ API ---
@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return JSONResponse({
        'status': 'healthy', 
        'simclr_model_loaded': model is not None,
        'ap10k_model_loaded': ap10k_model is not None
    })